{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ian-byrne/MADSmilestone2/blob/main/scores_cnn_resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa6L-qwlfHPf"
      },
      "source": [
        "# Supervised Learning: Clock Drawing Image Classification with Convolutional Neural Networks\n",
        "### Stacey Beck and Ian Byrne\n",
        "\n",
        "- Split data into sets of Training (x = image arrays ; y = labels), Test (~10% image arrays), and Validation (~10% of the Training). \n",
        "- Build CNN using Pytorch for Training and Test:\n",
        "  - Specify CUDA\n",
        "  - 2D convolution, Normalization (for faster training), Non-linear Activation Function (ex. RELU), Max Pooling (downsampling to reduce learned parameters).\n",
        "  - Define Layers \n",
        "  - Build Forward and backward pass\n",
        "  - Define optimizer (due to many - deep - nodes) ex) ADAM\n",
        "  - Calculate Loss (BCE)\n",
        "  - Calculate Accuracy, Precision, Recall (Confusion Matrix)\n",
        "  - Plot ROC and print Confusion Matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKcEhDBuf5ow",
        "outputId": "ca9c6010-1725-4bc1-f40c-fefd29a29d4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eoct7Or8ezZq",
        "outputId": "7a335483-61d4-49d4-c8bf-efa7f617c6c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'MADSmilestone2'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 718, done.\u001b[K\n",
            "remote: Counting objects: 100% (718/718), done.\u001b[K\n",
            "remote: Compressing objects: 100% (611/611), done.\u001b[K\n",
            "remote: Total 718 (delta 386), reused 229 (delta 97), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (718/718), 7.04 MiB | 38.00 KiB/s, done.\n",
            "Resolving deltas: 100% (386/386), done.\n",
            "/content/MADSmilestone2\n"
          ]
        }
      ],
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/ian-byrne/MADSmilestone2.git\n",
        "# Change directory into cloned repo\n",
        "%cd MADSmilestone2\n",
        "#!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Log111B8T-zg",
        "outputId": "e7095481-7a4b-49f1-8c52-dccb9be66d88"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtH9tfFdQepN",
        "outputId": "01114e98-db20-4bd9-d07a-0e6fdb448bd4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ],
      "source": [
        "# General Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import logging\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Custom Libraries\n",
        "import Loading.load_data as ld\n",
        "import ImagePlayground.Images\n",
        "\n",
        "# Pytroch Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchmetrics\n",
        "\n",
        "# To Evaluate model\n",
        "from tqdm import tqdm\n",
        "import torchmetrics\n",
        "from torchmetrics import ConfusionMatrix\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# To visualize model\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from skimage.io import imread\n",
        "\n",
        "# To split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5jwtXk7qv9-"
      },
      "source": [
        "# Build CNN Model using Pytorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nKizV2h4lMH"
      },
      "source": [
        "### Building and Training\n",
        "Architecture choices influenced from: \n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2018/12/guide-convolutional-neural-network-cnn/\n",
        "\n",
        "https://medium.datadriveninvestor.com/five-powerful-cnn-architectures-b939c9ddd57b\n",
        "\n",
        "https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7OOYf4s0smrq"
      },
      "outputs": [],
      "source": [
        "#epochs = 2\n",
        "train_batch_size = 8\n",
        "val_batch = 4\n",
        "test_batch = 1\n",
        "learning_rate = 1e-3\n",
        "kernel_size = 3\n",
        "stride = 1\n",
        "padding = 1 #2*floor(3/2)\n",
        "weight_decay = 1e-5\n",
        "# Define file extension to use for new data saves\n",
        "extension_ = \"res_netfp\"\n",
        "# Normalize data if rgb and set rgb_val to True to convert \n",
        "normalize_ = 'True'\n",
        "# Define which round to get data from\n",
        "rnd = 10 #1,2,3,4,6,7,8,9\n",
        "# Define model extensions for naming file (which model do we want to train on)\n",
        "model_ext = \"res_netfp\"\n",
        "# m = 'First model'\n",
        "# m = 'pre-trained'\n",
        "m = 'pre-trained-res'\n",
        "# m = 'resnet'\n",
        "\n",
        "# Define the path to images\n",
        "images_path = '/content/gdrive/MyDrive/Data/Nhats Dataset/NHATS_R11_ClockDrawings_V2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ypf_eiS8Eghk"
      },
      "outputs": [],
      "source": [
        "def open_dict(path):\n",
        "    \"\"\"Takes in path and opens file\"\"\"\n",
        "\n",
        "    cust_file = open(path, \"r\")\n",
        "    contents = cust_file.read()\n",
        "    dictionary = ast.literal_eval(contents)\n",
        "    cust_file.close()\n",
        "\n",
        "    return dictionary\n",
        "\n",
        "\n",
        "dictionarytr = open_dict(\n",
        "    \"/content/MADSmilestone2/Data/Dictionaries/score_dicts/tr_scor_dict_bal.txt\"\n",
        ")\n",
        "dictionaryv = open_dict(\n",
        "    \"/content/MADSmilestone2/Data/Dictionaries/score_dicts/val_scor_dict_nobal.txt\"\n",
        ")\n",
        "dictionaryts = open_dict(\n",
        "    \"/content/MADSmilestone2/Data/Dictionaries/score_dicts/tst_scor_dict_nobal.txt\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54Zv5Nsk-R-U",
        "outputId": "0efb96ef-7e84-4224-8e7e-f2afc32d0ff4"
      },
      "outputs": [],
      "source": [
        "for id, val in dictionarytr.items():\n",
        "  print(\"Round {} length is {}\".format(str(id), str(len(val))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUo2KZ_UYhYE"
      },
      "outputs": [],
      "source": [
        "class ResizedClocks():\n",
        "    # Resized clock drawing dataset\n",
        "\n",
        "    def __init__(self, round, round_labels, rgb=None, transform=None, images_path=None):\n",
        "\n",
        "        # Args:\n",
        "        # round (int): Round to grab images from.\n",
        "        # values (list of tuples): Corresponding values for the round.\n",
        "\n",
        "        self.round = round\n",
        "        self.vals = round_labels\n",
        "        self.images_path = images_path\n",
        "        self.transform = transform\n",
        "        self.rgb = rgb\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vals)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        spid = self.vals[idx][0]\n",
        "        label = torch.tensor(int(self.vals[idx][1]))\n",
        "        \n",
        "        # Construct the image path\n",
        "        img_path = os.path.join(self.images_path, f\"{spid}.tif\")\n",
        "        \n",
        "        try:\n",
        "            # Load image directly from the path\n",
        "            im = Image.open(img_path)\n",
        "\n",
        "            if self.rgb:\n",
        "                #print('rgb')\n",
        "                gray = im.convert('RGB')\n",
        "            \n",
        "            else:\n",
        "                #print('gray')\n",
        "                gray = im.convert('1')\n",
        "\n",
        "            resized = gray.resize((160, 207))  # 284, 368))#(2560, 3312))\n",
        "            # resized = gray.resize((512, 662))\n",
        "            im_arr = np.float32(np.array(resized))  # .astype(float)\n",
        "\n",
        "            if self.transform:\n",
        "                im_arr = self.transform(im_arr)\n",
        "\n",
        "            return im_arr, label\n",
        "\n",
        "        except (FileNotFoundError, IOError) as e:\n",
        "            # Return None if image file is not found or corrupted\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Cco7mnKoYwEX"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "  \"\"\"From pytorch - way to bypass corrupt or non-existent data\"\"\"\n",
        "  batch = list(filter(lambda x: x is not None, batch))\n",
        "  return torch.utils.data.dataloader.default_collate(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Gn8N9D4YZJcR"
      },
      "outputs": [],
      "source": [
        "# initialize transformation: data to tensor and normalize\n",
        "# Could probably resize using torch.transforms\n",
        "if normalize_ == \"True\":\n",
        "    processes = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    )\n",
        "    rgb_val = \"True\"\n",
        "else:\n",
        "    processes = transforms.ToTensor()\n",
        "    rgb_val = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Bz4e3izRZMJx"
      },
      "outputs": [],
      "source": [
        "train_set = ResizedClocks(rnd, dictionarytr[rnd], transform=processes, rgb=rgb_val, images_path=images_path)\n",
        "val_set = ResizedClocks(rnd, dictionaryv[rnd], transform=processes, rgb=rgb_val, images_path=images_path)\n",
        "test_set = ResizedClocks(rnd, dictionaryts[rnd], transform=processes, rgb=rgb_val, images_path=images_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FPc4Tg8yZPiE"
      },
      "outputs": [],
      "source": [
        "# Define Dataloaders for the network\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size = train_batch_size, shuffle = True, num_workers = 6, collate_fn=collate_fn) \n",
        "validate_loader = torch.utils.data.DataLoader(val_set, batch_size = val_batch, shuffle = True, num_workers = 6, collate_fn=collate_fn) #64, 8,1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kne45tSsQvFQ",
        "outputId": "8be78d6d-c768-4fdb-bd20-df005bc030e4"
      },
      "outputs": [],
      "source": [
        "# For round 10, there are some corrupt data that when batched at size 1 is not taken\n",
        "# care of by the collate function, but Nonechucks library skips the missing data and \n",
        "# moves on, replacing that missing data index with the next piece of data\n",
        "# could probably just use this in place of collate for all the loading\n",
        "if rnd == 10:\n",
        "  !pip install nonechucks\n",
        "  import nonechucks as nc\n",
        "  test_set_safe = nc.SafeDataset(test_set)\n",
        "  test_loader = torch.utils.data.DataLoader(test_set_safe, batch_size = test_batch, shuffle = False)\n",
        "else:\n",
        "  test_loader = torch.utils.data.DataLoader(test_set, batch_size = test_batch, shuffle = False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KAovKkRXFU4j"
      },
      "outputs": [],
      "source": [
        "def set_model(m, model_ext, device):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"First Model training on GPU\")\n",
        "\n",
        "        if m == \"First model\":\n",
        "            # Create model object\n",
        "            model = ConvNet()\n",
        "            model = model.to(device)  # (float).cuda()\n",
        "\n",
        "        elif m == \"pre-trained\":\n",
        "            mPATH = \"/content/gdrive/MyDrive/Colab Notebooks/Models/cnn_512_662.model{}\".format(\n",
        "                model_ext\n",
        "            )\n",
        "            model = ConvNet()\n",
        "            model.load_state_dict(torch.load(mPATH))\n",
        "            model.to(device)\n",
        "            print(\"New Model{} training on GPU\".format(model_ext))\n",
        "\n",
        "        elif m == \"resnet\":\n",
        "            model = models.resnet50(pretrained=True)\n",
        "            num_ftrs = model.fc.in_features\n",
        "            model.fc = nn.Linear(num_ftrs, 6)\n",
        "            model = model.to(device)\n",
        "            print(\"RESNET Model training on GPU\")\n",
        "\n",
        "        elif m == \"pre-trained-res\":\n",
        "            mPATH = \"/content/gdrive/MyDrive/Colab_Notebooks/cnn_1run.model{}\".format(\n",
        "                model_ext\n",
        "            )\n",
        "            model = models.resnet50()\n",
        "            num_ftrs = model.fc.in_features\n",
        "            model.fc = nn.Linear(num_ftrs, 6)\n",
        "            model.load_state_dict(torch.load(mPATH))\n",
        "            model.to(device)\n",
        "            print(\"New Model{} training on GPU\".format(model_ext))\n",
        "\n",
        "    else:\n",
        "        print(\"CUDA is not available. Turn on GPU\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XiY5mGPbZxDV"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_pred, y_test):\n",
        "  # Calculating model accuracy at each epoch \n",
        "  y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
        "  _, y_pred_prob = torch.max(y_pred_softmax, dim = 1)\n",
        "  correct_pred = (y_pred_prob == y_test).float()\n",
        "  acc = correct_pred.sum() / len(correct_pred)\n",
        "  acc = torch.round(acc * 100)\n",
        "\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WrUu9AXiZ0SL"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        # without considering batch size: Input shape : (None,368, 284, 1) , parameters: (3*3*1*16+16) = 160\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=1,\n",
        "            out_channels=16,  # one input channel gray scale, 16 filters out\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "        )  # Out:(None,386, 284, 16). ### TRY kernel 7x7 padding 3\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1\n",
        "        )  # params: (3*3*16*32+32) = 4640\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  # Out: (None, 184, 142, 32)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1\n",
        "        )  # params: (3*3*16*32+32) = 4640\n",
        "        self.conv4 = nn.Conv2d(\n",
        "            in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1\n",
        "        )  # params: (3*3*32*32+32) = 9248\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)  # Output shape = (None, 92, 71, 64)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # self.conv5 = nn.Conv2d(in_channels = 64, out_channels = 128,\n",
        "        # kernel_size = 3, stride = 1, padding = 1) # params: (3*3*32*32+32) = 9248\n",
        "        self.conv6 = nn.Conv2d(\n",
        "            in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1\n",
        "        )  # params: (3*3*32*32+32) = 9248\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)  # Output shape = (None, 46, 35, 128)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.do2 = nn.Dropout(0.3)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc1 = nn.Linear(\n",
        "            128 * 64 * 82, 60\n",
        "        )  # most recent original size of: 512, 662 -->64 x 82\n",
        "        self.do3 = nn.Dropout(0.4)  # 40 % probability\n",
        "        # self.fc3 = nn.Linear(60, 30)\n",
        "        self.fc2 = nn.Linear(60, 6)  # left with 3 for the three classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn1(self.pool1(F.relu(self.conv2(F.relu(self.conv1(x))))))\n",
        "        x = self.bn2(self.pool2(F.relu(self.conv4(F.relu(self.conv3(x))))))\n",
        "        # x = self.bn3(self.pool3(F.relu(self.conv6(F.relu(self.conv5(x))))))\n",
        "        x = self.bn3(self.pool3(F.relu(self.conv6((x)))))\n",
        "        x = self.do2(x)\n",
        "        x = x.view(x.size(0), 128 * 64 * 82)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.do3(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c26c09a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_val_model(epochs):\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        train_epoch_loss = 0\n",
        "        train_epoch_acc = 0\n",
        "\n",
        "        # set model in training mode\n",
        "        model.train()\n",
        "        print(\"\\nEpoch$ : %d\" % epoch)\n",
        "        for x_train_batch, y_train_batch in tqdm(train_loader):\n",
        "            x_train_batch = x_train_batch.to(\n",
        "                device\n",
        "            )  # (float).to(device) # for GPU support\n",
        "            y_train_batch = y_train_batch.to(device)\n",
        "\n",
        "            # print(x_train_batch.shape)\n",
        "\n",
        "            # sets gradients to 0 to prevent interference with previous epoch\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass through NN\n",
        "            y_train_pred = model(x_train_batch)  # .to(float)\n",
        "            train_loss = criterion(y_train_pred, y_train_batch)\n",
        "            train_acc = accuracy(y_train_pred, y_train_batch)\n",
        "\n",
        "            # Backward pass, updating weights\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            train_epoch_loss += train_loss.item()\n",
        "            train_epoch_acc += train_acc.item()\n",
        "\n",
        "        with torch.set_grad_enabled(False):\n",
        "            val_epoch_loss = 0\n",
        "            val_epoch_acc = 0\n",
        "\n",
        "            model.eval()\n",
        "            for x_val_batch, y_val_batch in tqdm(validate_loader):\n",
        "\n",
        "                x_val_batch = x_val_batch.to(device)  # .to(float)\n",
        "                y_val_batch = y_val_batch.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                y_val_pred = model(x_val_batch)  # .to(float)\n",
        "                val_loss = criterion(y_val_pred, y_val_batch)\n",
        "                val_acc = accuracy(y_val_pred, y_val_batch)\n",
        "\n",
        "                val_epoch_loss += val_loss.item()\n",
        "                val_epoch_acc += val_acc.item()\n",
        "\n",
        "        # Prevent plateauing validation loss\n",
        "        # scheduler.step(val_epoch_loss/len(validate_loader))\n",
        "\n",
        "        loss_stats[\"train\"].append(train_epoch_loss / len(train_loader))\n",
        "        loss_stats[\"val\"].append(val_epoch_loss / len(validate_loader))\n",
        "        accuracy_stats[\"train\"].append(train_epoch_acc / len(train_loader))\n",
        "        accuracy_stats[\"val\"].append(val_epoch_acc / len(validate_loader))\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch+0:03}: Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(validate_loader):.5f}\"\n",
        "        )\n",
        "        print(\n",
        "            f\"Train Acc: {train_epoch_acc/len(train_loader):.3f} | Val Acc: {val_epoch_acc/len(validate_loader):.3f}\"\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59d7dc2f",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02e551b7",
      "metadata": {},
      "outputs": [],
      "source": [
        " # Set to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36de9489",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get model\n",
        "model = set_model(m, model_ext, device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)#, momentum = 0.9) #or ADAM/ momentum\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "# scheduler = lr_scheduler.StepLR(optimizer, step_size = 4, gamma=0.1)\n",
        "scheduler = ReduceLROnPlateau(optimizer, \"min\", patience=4)\n",
        "\n",
        "accuracy_stats = {\"train\": [], \"val\": []}\n",
        "\n",
        "loss_stats = {\"train\": [], \"val\": []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "680ab051",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_val_model(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4917330",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save GPU model\n",
        "model_name = 'cnn_1run.model{}'.format(extension_)\n",
        "PATH = \"/content/gdrive/MyDrive/Colab_Notebooks/{}\".format(model_name)\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87cffebe",
      "metadata": {},
      "source": [
        "# Visualize the Training and Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4978c445",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataframes\n",
        "train_val_acc_df = pd.DataFrame.from_dict(accuracy_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
        "train_val_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
        "train_val_acc_df.to_csv('/content/gdrive/MyDrive/Colab_Notebooks/acc{}.csv'.format(extension_), index = False)\n",
        "train_val_loss_df.to_csv('/content/gdrive/MyDrive/Colab_Notebooks/loss{}.csv'.format(extension_), index = False)\n",
        "# Plot the dataframes\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,7))\n",
        "sns.lineplot(data=train_val_acc_df, x = \"epochs\", y=\"value\", hue=\"variable\",  ax=axes[0]).set_title('Train-Val Accuracy/Epoch')\n",
        "sns.lineplot(data=train_val_loss_df, x = \"epochs\", y=\"value\", hue=\"variable\", ax=axes[1]).set_title('Train-Val Loss/Epoch')\n",
        "fig.savefig('/content/gdrive/MyDrive/Colab_Notebooks/acc_loss{}.png'.format(extension_))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "760b42ad",
      "metadata": {},
      "source": [
        "# Evaluate the model using Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08e94ea8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate performance\n",
        "y_test = torch.tensor([])\n",
        "test_acc = torchmetrics.Accuracy()\n",
        "\n",
        "with torch.set_grad_enabled(False):\n",
        "  model.eval()\n",
        "  #model.to(float)\n",
        "  for batches in tqdm(test_loader):\n",
        "    x_test, y_test = batches\n",
        "    x_test = x_test.to(device)\n",
        "    y_test = y_test.to(device)\n",
        "    y_pred = model(x_test)\n",
        "    test_acc(y_pred.cpu(), y_test.cpu())\n",
        "    total_test_acc = test_acc.compute()\n",
        "  print('\\nTest Acc: ', total_test_acc)\n",
        "  test_acc.reset()\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "311cc5fe",
      "metadata": {},
      "source": [
        "## Create Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22410c16",
      "metadata": {},
      "outputs": [],
      "source": [
        "all_pred = []\n",
        "all_preds = torch.tensor([])\n",
        "y_test = torch.tensor([])\n",
        "with torch.set_grad_enabled(False):\n",
        "  model.eval()\n",
        "  for x_test_batch, y_test_batch in tqdm(test_loader):\n",
        "    x_test_batch = x_test_batch.to(device)#.to(float).to(device)\n",
        "    y_test_pred = model(x_test_batch)\n",
        "    _, y_pred_probs = torch.max(y_test_pred, dim = 1)\n",
        "    all_pred.append(y_pred_probs.cpu().numpy())\n",
        "    all_preds = torch.cat((all_preds.cpu(), y_pred_probs.cpu()),dim = 0)\n",
        "    y_test = torch.cat((y_test, y_test_batch), dim = 0) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9584210",
      "metadata": {},
      "outputs": [],
      "source": [
        "confusion_matrix_df = pd.DataFrame(confusion_matrix(y_test, all_pred))#.rename(columns=idx2class, index=idx2class)\n",
        "sns.heatmap(confusion_matrix_df, annot=True, fmt=\".2f\", cmap='BuGn')\n",
        "plt.xlabel(\"prediction\")\n",
        "plt.ylabel(\"label (ground truth)\")\n",
        "plt.savefig('/content/gdrive/MyDrive/Colab_Notebooks/CMTX{}.png'.format(extension_))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3fac1e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "#class_names = [\"Possible Dementia\", \"Likely Dementia\", \"No Dementia\"]\n",
        "class_vals = [0,1,2,3,4,5]\n",
        "\n",
        "cr = classification_report(y_test, all_pred, class_vals, output_dict = True)\n",
        "try:\n",
        "    cr_file = open('/content/gdrive/MyDrive/Colab_Notebooks/cr{}.txt'.format(extension_), 'wt')\n",
        "    cr_file.write(str(cr))\n",
        "    cr_file.close()\n",
        "  \n",
        "except:\n",
        "    print(\"Unable to write to file\")\n",
        "print(classification_report(y_test, all_pred, class_vals))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ian-byrne/MADSmilestone2/blob/main/Transfer_Learning_Kmeans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bkxj5ev_zKj",
        "outputId": "d081ea47-dcd1-4d4e-befc-7b98d584ef18"
      },
      "outputs": [],
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/ian-byrne/MADSmilestone2.git\n",
        "\n",
        "# Change directory into cloned repo\n",
        "%cd MADSmilestone2\n",
        "\n",
        "# List repo contents\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM664aismOoy",
        "outputId": "8ee69a7d-11fc-4bcc-f239-605261d07ffb"
      },
      "outputs": [],
      "source": [
        "!pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlOtXXtK8so8",
        "outputId": "dbb6c093-8c16-464b-b832-15d1023582ba"
      },
      "outputs": [],
      "source": [
        "!pip install boto3 torchmetrics\n",
        "from tqdm import tqdm\n",
        "import boto3\n",
        "import io\n",
        "import logging\n",
        "import os\n",
        "import ast\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import Loading.load_data as ld\n",
        "\n",
        "\n",
        "# Pytroch Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchmetrics\n",
        "from torch.autograd import Variable\n",
        "import botocore\n",
        "\n",
        "import tempfile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QsWRQUX6IeJ",
        "outputId": "6d736856-3541-47e6-8c26-0da65d53382e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "a587eJhXj6AN"
      },
      "outputs": [],
      "source": [
        "path = '/content/MADSmilestone2/Data/Dictionaries/dementia_label_dicts/train_ai.txt'\n",
        "cust_file = open(path, \"r\")\n",
        "#print(cust_file.readline())\n",
        "contents = cust_file.read() \n",
        "dictionarytr = ast.literal_eval(contents)\n",
        "cust_file.close()\n",
        "\n",
        "path1 = '/content/MADSmilestone2/Data/Dictionaries/dementia_label_dicts/val_dict_nhat.txt'\n",
        "cust_file = open(path1, \"r\")\n",
        "#print(cust_file.readline())\n",
        "contents = cust_file.read()\n",
        "dictionaryv = ast.literal_eval(contents)\n",
        "cust_file.close()\n",
        "\n",
        "path2 = '/content/MADSmilestone2/Data/Dictionaries/dementia_label_dicts/test_dict_nhat.txt'\n",
        "cust_file = open(path2, \"r\")\n",
        "#print(cust_file.readline())\n",
        "contents = cust_file.read()\n",
        "dictionaryts = ast.literal_eval(contents)\n",
        "cust_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "jbMI4s9hk5QE"
      },
      "outputs": [],
      "source": [
        "# Define some of the loader variables\n",
        "train_batch_size = 1\n",
        "val_batch = 4\n",
        "test_batch = 1\n",
        "#rgb_val = None\n",
        "rnd = 7\n",
        "\n",
        "normalize_ = True\n",
        "numpy_ = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vv1Ta24tKsP"
      },
      "outputs": [],
      "source": [
        "!pip install graphviz\n",
        "!pip install hiddenlayer\n",
        "import torch\n",
        "import torchvision.models\n",
        "import hiddenlayer as hl\n",
        "\n",
        "\n",
        "# Build HiddenLayer graph\n",
        "# Jupyter Notebook renders it automatically\n",
        "hl.build_graph(model.to(device), torch.zeros([512, 662]).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAA3aUxbkiKO"
      },
      "outputs": [],
      "source": [
        "class ResizedClocks():\n",
        "    # Resized clock drawing dataset\n",
        "\n",
        "    def __init__(self, round, round_labels, rgb=None, transform=None, images_path=None):\n",
        "\n",
        "        # Args:\n",
        "        # round (int): Round to grab images from.\n",
        "        # values (list of tuples): Corresponding values for the round.\n",
        "\n",
        "        self.round = round\n",
        "        self.vals = round_labels\n",
        "        self.images_path = images_path\n",
        "        self.transform = transform\n",
        "        self.rgb = rgb\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vals)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        spid = self.vals[idx][0]\n",
        "        label = torch.tensor(int(self.vals[idx][1]))\n",
        "        \n",
        "        # Construct the image path\n",
        "        img_path = os.path.join(self.images_path, f\"{spid}.tif\")\n",
        "        \n",
        "        try:\n",
        "            # Load image directly from the path\n",
        "            im = Image.open(img_path)\n",
        "\n",
        "            if self.rgb:\n",
        "                #print('rgb')\n",
        "                gray = im.convert('RGB')\n",
        "            \n",
        "            else:\n",
        "                #print('gray')\n",
        "                gray = im.convert('1')\n",
        "\n",
        "            resized = gray.resize((160, 207))  # 284, 368))#(2560, 3312))\n",
        "            # resized = gray.resize((512, 662))\n",
        "            im_arr = np.float32(np.array(resized))  # .astype(float)\n",
        "\n",
        "            if self.transform:\n",
        "                im_arr = self.transform(im_arr)\n",
        "\n",
        "            return im_arr, label\n",
        "\n",
        "        except (FileNotFoundError, IOError) as e:\n",
        "            # Return None if image file is not found or corrupted\n",
        "            return None\n",
        "def collate_fn(batch):\n",
        "  \"\"\"From pytorch - way to bypass corrupt or non-existent data\"\"\"\n",
        "  batch = list(filter(lambda x: x is not None, batch))\n",
        "  return torch.utils.data.dataloader.default_collate(batch)\n",
        "# initialize transformation: data to tensor and normalize\n",
        "# Could probably resize using torch.transforms\n",
        "if normalize_ == \"True\":\n",
        "    processes = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    )\n",
        "    rgb_val = \"True\"\n",
        "else:\n",
        "    processes = transforms.ToTensor()\n",
        "    rgb_val = None\n",
        "\n",
        "\n",
        "\n",
        "# Define which round to get data from\n",
        "if numpy_ == False:\n",
        "  # Get the data from S3 and perform transformations\n",
        "  train_set = ResizedClocks(rnd, dictionarytr[rnd], transform=processes, rgb=rgb_val, images_path=images_path)\n",
        "  val_set = ResizedClocks(rnd, dictionaryv[rnd], transform=processes, rgb=rgb_val, images_path=images_path)\n",
        "  test_set = ResizedClocks(rnd, dictionaryts[rnd], transform=processes, rgb=rgb_val, images_path=images_path)\n",
        "\n",
        "\n",
        "# Define Dataloaders for the network\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size = train_batch_size, shuffle = True, num_workers = 6, collate_fn=collate_fn) \n",
        "validate_loader = torch.utils.data.DataLoader(val_set, batch_size = val_batch, shuffle = True, num_workers = 6, collate_fn=collate_fn) #64, 8,1\n",
        "\n",
        "\n",
        "# For round 10, there are some corrupt data that when batched at size 1 is not taken\n",
        "# care of by the collate function, but Nonechucks library skips the missing data and \n",
        "# moves on, replacing that missing data index with the next piece of data\n",
        "# could probably just use this in place of collate for all the loading\n",
        "if rnd == 10:\n",
        "  !pip install nonechucks\n",
        "  import nonechucks as nc\n",
        "  test_set_safe = nc.SafeDataset(test_set)\n",
        "  test_loader = torch.utils.data.DataLoader(test_set_safe, batch_size = test_batch, shuffle = False)\n",
        "\n",
        "else:\n",
        "  test_loader = torch.utils.data.DataLoader(test_set, batch_size = test_batch, shuffle = False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6m6xdD8plDQ"
      },
      "outputs": [],
      "source": [
        "#original size: 2560, 3312\n",
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    \n",
        "    # without considering batch size: Input shape : (None,368, 284, 1) , parameters: (3*3*1*16+16) = 160\n",
        "    self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 16, # one input channel gray scale, 16 filters out\n",
        "                            kernel_size = 3, stride = 1, padding = 1) #Out:(None,386, 284, 16). ### TRY kernel 7x7 padding 3\n",
        "    self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, \n",
        "                          kernel_size = 3, stride = 1, padding = 1) #params: (3*3*16*32+32) = 4640                        \n",
        "    self.pool1 = nn.MaxPool2d(2, 2) #Out: (None, 184, 142, 32) \n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "    self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, \n",
        "                          kernel_size = 3, stride = 1, padding = 1) #params: (3*3*16*32+32) = 4640    \n",
        "    self.conv4 = nn.Conv2d(in_channels = 64, out_channels = 64, \n",
        "                          kernel_size = 3, stride = 1, padding = 1) # params: (3*3*32*32+32) = 9248                    \n",
        "    self.pool2 = nn.MaxPool2d(2, 2) #Output shape = (None, 92, 71, 64) \n",
        "    self.bn2 = nn.BatchNorm2d(64)  \n",
        "\n",
        "    #self.conv5 = nn.Conv2d(in_channels = 64, out_channels = 128, \n",
        "                          #kernel_size = 3, stride = 1, padding = 1) # params: (3*3*32*32+32) = 9248 \n",
        "    self.conv6 = nn.Conv2d(in_channels = 64, out_channels = 128, \n",
        "                          kernel_size = 3, stride = 1, padding = 1) # params: (3*3*32*32+32) = 9248\n",
        "    self.pool3 = nn.MaxPool2d(2, 2) #Output shape = (None, 46, 35, 128) \n",
        "    self.bn3 = nn.BatchNorm2d(128)\n",
        "    self.do2 = nn.Dropout(0.3)\n",
        "                                   \n",
        "                             \n",
        "    # Fully connected layer\n",
        "    self.fc1 = nn.Linear(128*64*82,60) #most recent original size of: 512, 662 -->64 x 82\n",
        "    self.do3 = nn.Dropout(0.4) #40 % probability  \n",
        "    #self.fc3 = nn.Linear(60, 30)\n",
        "    self.fc2 = nn.Linear(60, 3) # left with 3 for the three classes                     \n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.bn1(self.pool1(F.relu(self.conv2(F.relu(self.conv1(x))))))\n",
        "    x = self.bn2(self.pool2(F.relu(self.conv4(F.relu(self.conv3(x))))))\n",
        "    #x = self.bn3(self.pool3(F.relu(self.conv6(F.relu(self.conv5(x))))))\n",
        "    x = self.bn3(self.pool3(F.relu(self.conv6((x)))))\n",
        "    x = self.do2(x)\n",
        "    x = x.view(x.size(0),128*64*82)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.do3(x)\n",
        "    x = self.fc2(x)\n",
        "    return x              "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rAWdxiWkyyU"
      },
      "source": [
        "# Resnet50 + KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-b8Nx4kUrug",
        "outputId": "2c84d572-2dab-4e65-8a3a-149f5a3f7d26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "res 50 pretrained\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "# Set to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "mod = 'res50'\n",
        "\n",
        "if mod == 'res':\n",
        "  model_ext = '4_hat'\n",
        "  mPATH = '/content/gdrive/MyDrive/Colab Notebooks/Models/cnn_512_662.model{}'.format(model_ext)\n",
        "  model = models.resnet50()#'pretrained')\n",
        "  num_ftrs = model.fc.in_features\n",
        "  model.fc = nn.Linear(num_ftrs, 3)\n",
        "  model.load_state_dict(torch.load(mPATH))\n",
        "  model.to(device)\n",
        "  print(\"Res half data\")\n",
        "\n",
        "if mod == 'conv':\n",
        "  model_ext = '4_fix'\n",
        "  mPATH = '/content/gdrive/MyDrive/Colab Notebooks/Models/cnn_512_662.model{}'.format(model_ext)\n",
        "  model = ConvNet()\n",
        "  #model.load_state_dict(torch.load(mPATH, map_location=torch.device('cpu')))\n",
        "  model.load_state_dict(torch.load(mPATH))\n",
        "  model.to(device)\n",
        "  print(\"conv\")\n",
        "\n",
        "if mod == 'res50':\n",
        "  model = models.resnet50(pretrained = True)\n",
        "  num_ftrs = model.fc.in_features\n",
        "  model.fc = nn.Linear(num_ftrs, 3)\n",
        "  model = model.to(device)\n",
        "  print(\"res 50 pretrained\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKfY7L8urOZT",
        "outputId": "fa9087a3-9e37-4fdf-bfb1-6f0096636173"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "Ij-c1MqHAFzc"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "# Remove prediction layer (last fc layer)\n",
        "model_1 = nn.Sequential(*list(model.children())[:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPSmio_aA2RF",
        "outputId": "995c8908-b350-4bff-ea03-6fbc9e4375af"
      },
      "outputs": [],
      "source": [
        "# Use model for evaluation\n",
        "model_1.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GDqbInOVXbO"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "\n",
        "for x, lb in tqdm(test_loader):\n",
        "  x = x.to(device)\n",
        "  preds = model_1(x)\n",
        "  X.append(preds.cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCrc8SzhOZXL",
        "outputId": "a530b726-0690-4977-caaa-6c25fa78e016"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 2048, 1, 1])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaI9qENXoBXV",
        "outputId": "c0426d5a-d8a0-4ae8-c4cb-a75c2b1f10cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape:  (254, 1, 2048, 1, 1)\n"
          ]
        }
      ],
      "source": [
        "x_ = np.array([t.cpu().detach().numpy() for t in X])\n",
        "#x_ = np.array([t for t in X])\n",
        "print(\"shape: \", x_.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0L28x_VO4hN",
        "outputId": "3f1161d8-e33a-4598-aa38-8a98687f631b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(254, 2048)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w = x_.reshape(x_.shape[0], -1)\n",
        "w.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEf9iFTxSaJK",
        "outputId": "c8d04e03-6a97-4181-db46-52faaccd6215"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of components before PCA  = 2048\n",
            "Number of components after PCA 0.98 = 114\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "# Make an instance of the Model\n",
        "variance = 0.98 #The higher the explained variance the more accurate the model will remain, but more dimensions will be present\n",
        "pca = PCA(variance)\n",
        "\n",
        "pca.fit(w) #fit the data according to our PCA instance\n",
        "print(\"Number of components before PCA  = \" + str(w.shape[1]))\n",
        "print(\"Number of components after PCA 0.98 = \" + str(pca.n_components_)) \n",
        "#dimension reduced from 784\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2fA1SXiS7pX",
        "outputId": "62c855c9-f521-45ce-c1f1-2acb6542ec82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dim after PCA:  (254, 114)\n"
          ]
        }
      ],
      "source": [
        "w_pca = pca.transform(w)\n",
        "print(\"Dim after PCA: \", w_pca.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "TZB7kHJKPc1J"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "sil_score = []\n",
        "kl = []\n",
        "k_max = 9\n",
        "\n",
        "for k in range(2, k_max+1):\n",
        "  kmeans2 = KMeans(n_clusters = k).fit(w)#w)\n",
        "  labels = kmeans2.labels_\n",
        "  sil_score.append(silhouette_score(w, labels, metric = 'euclidean'))\n",
        "  kl.append(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "0YyDD2knPkOb",
        "outputId": "33e8bbe0-c644-42ff-b52a-d2c56d2ba495"
      },
      "outputs": [],
      "source": [
        "sil_name = '50res_fit7_hatsil.png'\n",
        "plt.plot(kl, sil_score)\n",
        "plt.ylabel('Silhoutte Score')\n",
        "plt.xlabel('Best K for KMeans')\n",
        "#plt.title('PCA dim from 2048 to 114')\n",
        "plt.title('fit to round 7')\n",
        "plt.suptitle('ResNet pretrained on ImageNet')\n",
        "#plt.title('Res trained on 50% dataset and fit to Round 7')\n",
        "plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/model_charts/{}'.format(sil_name))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiIZUsepvvsY"
      },
      "source": [
        "# Apply optimal K with clustering for labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMI9NAVNvuzD",
        "outputId": "a9865182-b8d3-4ff2-f8cd-c6df2045acb0"
      },
      "outputs": [],
      "source": [
        "x_trainK = []\n",
        "y_train = []\n",
        "# Pulling in data from round 3 to predict labels\n",
        "# Since we predicted optimal clusters with resnet50 trained on rounds 1, 5, 6, 7\n",
        "for x, lb in tqdm(train_loader):\n",
        "  x_trainK.append(x)\n",
        "  y_train.append(lb)\n",
        "  #preds = model(x)\n",
        "  #make_dot(preds.mean(), params=dict(model.named_parameters()), show_attrs=True, show_saved=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzMEqyKXzBYh",
        "outputId": "4eaecb28-dfe0-40f6-8da0-352c8ff94339"
      },
      "outputs": [],
      "source": [
        "#y_train_ = np.array([t.numpy() for t in y_train])\n",
        "y_train_ = np.array(y_train)\n",
        "print(\"shape: \", (y_train_).shape)\n",
        "print(type(y_train_))\n",
        "y_train_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEDM8daRxx1M",
        "outputId": "05d51cb4-ce85-4ffb-9707-6a91667c4ea2"
      },
      "outputs": [],
      "source": [
        "x_train_ = np.array([t.numpy() for t in x_trainK])\n",
        "print(\"shape: \", x_train_.shape)\n",
        "print(type(x_train_))\n",
        "x_train_[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFgTI7rxx16i",
        "outputId": "d784cccb-d73f-4dab-8322-344dce563512"
      },
      "outputs": [],
      "source": [
        "X_train = x_train_.reshape(x_train_.shape[0], -1) # reshaped to # samples and flattened dims\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "6TKYfc58Wuqv"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "variance = 0.98 #The higher the explained variance the more accurate the model will remain, but more dimensions will be present\n",
        "pca = PCA(variance)\n",
        "pca.fit(X_train)\n",
        "pct = pca.transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IujNYx435Za",
        "outputId": "bd88ea78-511a-4e50-bcc6-d7be9ad30efc"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters = 3, random_state = 0).fit(X_train)\n",
        "kmeans.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipt3mabFrjXe",
        "outputId": "928085f3-a84a-4b98-e7b4-0e82045e5419"
      },
      "outputs": [],
      "source": [
        "y_train_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzujGb7d8X0Z",
        "outputId": "75f6d2ca-193d-4375-b906-311d2e3aaed3"
      },
      "outputs": [],
      "source": [
        "kmeans.labels_.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdwrycO47H0v",
        "outputId": "6a7b0724-49df-4cae-94ac-9b67eea429a6"
      },
      "outputs": [],
      "source": [
        "clusters = kmeans.fit_predict(X_train)\n",
        "kmeans.cluster_centers_.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_JPqyFasRV9",
        "outputId": "e035b0d7-0839-4715-872f-7ca69fb176a9"
      },
      "outputs": [],
      "source": [
        "correct_pred = (clusters == y_train_)\n",
        "  #acc = correct_pred.sum() / len(correct_pred)\n",
        "print(correct_pred)\n",
        "print(correct_pred.sum())\n",
        "len(correct_pred)\n",
        "acc = correct_pred.sum()/len(correct_pred)\n",
        "acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Guv2Y8RcAP5U"
      },
      "source": [
        "Using NHATS label for round 7 fit on Pretrained ReNet50 on ImageNet only using 3 clusters was 79% accurate compared to cluster labels \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        },
        "id": "uyZdygGkFIjT",
        "outputId": "e7c6d2c2-c6f6-4543-f3e8-41f606c149d6"
      },
      "outputs": [],
      "source": [
        "#y_pred = kmeans.predict(X_train)\n",
        "fig = plt.figure(figsize = (15,15))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "plt.scatter(X_train[clusters == 0,0],X_train[clusters == 0,1],s = 50, c = 'green', label = \"cluster 1\")\n",
        "plt.scatter(X_train[clusters == 1,0],X_train[clusters == 1,1],s = 50, c = 'blue', label = \"cluster 2\")\n",
        "plt.scatter(X_train[clusters == 2,0],X_train[clusters == 2,1],s = 50, c = 'pink', label = \"cluster 3\")\n",
        "plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1], s = 100, c = \"red\", label = \"centroids\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5odr_oB1dk6y"
      },
      "source": [
        "# With Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poSPnQBmiOfr",
        "outputId": "a29bb3df-19e6-4734-96d3-835e7f2df58d"
      },
      "outputs": [],
      "source": [
        "keras_x = []\n",
        "\n",
        "for xk, _ in tqdm(test_loader):\n",
        "  keras_x.append(xk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzowqjfHHu-c",
        "outputId": "e800d917-0db1-4466-adfa-81f706e43072"
      },
      "outputs": [],
      "source": [
        "print(\"shape of keras_x: \", keras_x[0].shape)\n",
        "karr = np.array([t.cpu().detach().numpy() for t in keras_x])\n",
        "print(\"shape: \", karr.shape)\n",
        "im1= karr.reshape(len(keras_x), -1, 3)\n",
        "print('shape: ', im1.shape)\n",
        "print(\"dtype: \", im1.dtype)\n",
        "print(\"type: \", type(im1))\n",
        "\n",
        "img1 = Image.fromarray(im1, 'RGB')\n",
        "im_arr2 = np.float32(np.array(img1))\n",
        "im_arr2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u9biOO3jPtX",
        "outputId": "2bcad149-2a49-4c80-aca3-f677dcf7837a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf \n",
        "model_keras = tf.keras.applications.MobileNetV2(include_top=False,\n",
        "weights= 'imagenet', input_shape=(207, 160, 3))\n",
        "\n",
        "print(\"Image shape before model: \", im_arr2.shape)\n",
        "predictions1 = model_keras.predict(im_arr2.reshape(-1, 207, 160, 3))\n",
        "print(\"preds shape after model: \", predictions1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_fwz4phkAIN",
        "outputId": "19239515-01e6-4940-d971-cb3754999bfd"
      },
      "outputs": [],
      "source": [
        "pred_images1 = predictions1.reshape(im_arr2.shape[0], -1)\n",
        "print(\"Shape of prediction images: \", pred_images1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhgx0XKEXUt0"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "sil_score = []\n",
        "kl = []\n",
        "k_max = 9\n",
        "\n",
        "for k in range(2, k_max+1):\n",
        "  kmeans2 = KMeans(n_clusters = k).fit(pred_images1)\n",
        "  labels = kmeans2.labels_\n",
        "  sil_score.append(silhouette_score(pred_images1, labels, metric = 'euclidean'))\n",
        "  kl.append(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "CjqaCNUv8Fp-",
        "outputId": "4cb71e0b-30e0-4223-8494-27b75a08d46c"
      },
      "outputs": [],
      "source": [
        "plt.plot(kl, sil_score)\n",
        "plt.ylabel('Silhoutte Score')\n",
        "plt.ylabel('K')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNWYZdc2EAt0MatL2m2UGcr",
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "Copy of Transfer Learning Kmeans.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
